{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "- A Convolutional Neural Network (CNN) is a specialized type of deep neural network designed primarily for image data (though also used in speech, video, and text). CNNs automatically and adaptively learn spatial hierarchies of features (edges, textures, objects) through convolutional operations, reducing the need for manual feature extraction.\n",
        "- Difference :\n",
        "1. Connections :\n",
        "- ANN/MLP: Each neuron is connected to every neuron in the next layer (dense connections).\n",
        "- CNN: Neurons connect only to a small local region of the input (receptive field).\n",
        "2. Parameters :\n",
        "- ANN/MLP: Requires a large number of parameters when input size is high (e.g., images).\n",
        "- CNN: Has fewer parameters because the same filters (kernels) are shared across the image.\n",
        "3. Feature Extraction\n",
        "- ANN/MLP: Often requires manual feature engineering before training.\n",
        "- CNN: Performs automatic feature extraction using convolutional filters.\n",
        "4. Spatial Information\n",
        "- ANN/MLP: Treats all inputs as independent features, ignoring spatial locality.\n",
        "- CNN: Preserves spatial relationships (nearby pixels stay related in convolution).\n",
        "5. Scalability\n",
        "- ANN/MLP: Not suitable for high-dimensional data like large images (becomes computationally expensive).\n",
        "- CNN: Highly scalable and efficient for image, video, and other structured data.\n",
        "6. Performance on Images\n",
        "- ANN/MLP: Performs poorly on image recognition tasks due to lack of spatial awareness.\n",
        "- CNN: Achieves high accuracy in image classification, object detection, and segmentation.\n",
        "7. Architecture Structure\n",
        "- ANN/MLP: Made up of only fully connected layers.\n",
        "- CNN: Includes convolutional layers, pooling layers, and finally fully connected layers for classification.\n",
        "\n",
        "Question 2: Discuss the architecture of LeNet-5 and explain how it laid the foundation for modern deep learning models in computer vision. Include references to its original research paper.\n",
        "- LeNet-5 is one of the earliest Convolutional Neural Networks (CNNs), proposed by Yann LeCun et al. (1998) in the paper “Gradient-Based Learning Applied to Document Recognition” (LeCun et al., 1998). It was designed primarily for handwritten digit recognition (MNIST dataset).\n",
        "- Architecture of LeNet-5 (1998):\n",
        "1. Input Layer\n",
        " - Input: 32 × 32 grayscale image.MNIST digits (28×28) were zero-padded to fit this input size.\n",
        "2. C1 – Convolutional Layer\n",
        " - 6 filters (kernels) of size 5 × 5.Output: 28 × 28 × 6 feature maps.Detects simple local features (edges, corners).\n",
        "3. S2 – Subsampling (Pooling) Layer\n",
        " - Average pooling with 2 × 2 filters and stride 2.Output: 14 × 14 × 6.Reduces dimensionality while preserving important features.\n",
        "4. C3 – Convolutional Layer\n",
        " - 16 filters of size 5 × 5.Output: 10 × 10 × 16 feature maps.More complex features extracted.\n",
        "5. S4 – Subsampling Layer\n",
        " - Average pooling with 2 × 2 filters.Output: 5 × 5 × 16.\n",
        "6. C5 – Convolutional Layer\n",
        " - 120 filters of size 5 × 5 (fully connected to the previous layer).Output: 1 × 1 × 120 (flattened).\n",
        "7. F6 – Fully Connected Layer\n",
        " - 84 neurons (inspired by biological neurons).Activation function: tanh.\n",
        "8. Output Layer\n",
        " - 10 neurons (for digits 0–9).Activation: softmax (for classification).\n",
        "- How LeNet-5 Laid the Foundation for Modern Deep Learning:\n",
        "- Introduction of Convolution + Pooling\n",
        "  - Showed that CNNs can automatically extract hierarchical features from images.Reduced dependency on manual feature engineering.\n",
        "- Weight Sharing\n",
        "  - Convolutional filters are shared across spatial positions → drastically reduced number of parameters.Enabled training on limited computational resources (1990s hardware).\n",
        "- Hierarchical Feature Learning\n",
        "  - From simple edges in early layers → complex shapes in deeper layers.Concept still used in modern CNNs (AlexNet, VGG, ResNet).\n",
        "- Generalization to Vision Tasks\n",
        "  - Though trained on handwritten digits, the idea extended to image classification, object detection, and recognition.\n",
        "- Foundation for Modern CNNs\n",
        "- AlexNet (2012), which revived deep learning, was directly inspired by LeNet-5’s design.Today’s architectures (ResNet, EfficientNet, Vision Transformers) still rely on concepts introduced by LeNet-5.\n",
        "\n",
        "Question 3: Compare and contrast AlexNet and VGGNet in terms of design principles, number of parameters, and performance. Highlight key innovations and limitations of each.\n",
        "1. Year & Contribution\n",
        "- AlexNet (2012)\n",
        " - Proposed by Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton.Won ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 by a huge margin.Marked the revival of deep learning in computer vision.\n",
        "- VGGNet (2014)\n",
        "  - Proposed by Karen Simonyan and Andrew Zisserman (Oxford VGG group).Focused on studying the effect of network depth on performance.Popularized the use of very deep networks with small filters (3×3).\n",
        "2. Design Principles\n",
        "- AlexNet:\n",
        " - 8 layers (5 convolutional + 3 fully connected).Used large filters (11×11, 5×5) in early layers.Used ReLU activation, which sped up training compared to sigmoid/tanh.Used Dropout for regularization and Data Augmentation (image translation, reflection).Introduced GPU training for large-scale models.\n",
        "- VGGNet:\n",
        " - 16–19 layers (very deep compared to AlexNet).Used only 3×3 convolution filters, stacked to simulate larger receptive fields.Used 2×2 max pooling for downsampling.Fully connected layers at the end (similar to AlexNet).\n",
        "Emphasized depth and simplicity as design choice.\n",
        "3. Number of Parameters\n",
        "- AlexNet: ~ 60 million parameters.\n",
        "- VGGNet (VGG-16): ~ 138 million parameters (much heavier and memory intensive).\n",
        "4. Performance (ImageNet Classification)\n",
        "- AlexNet (2012):Top-5 error rate: 15.3%.First to dramatically outperform traditional computer vision methods.\n",
        "- VGGNet (2014):Top-5 error rate: 7.3% (significant improvement).Depth allowed capturing more complex patterns.\n",
        "5. Key Innovations\n",
        "- AlexNet:First large-scale CNN for ImageNet.Introduced ReLU activation and Dropout.Showed importance of GPU acceleration.\n",
        "- VGGNet:Introduced the idea of using small 3×3 filters repeatedly instead of large ones.Demonstrated that network depth is crucial for performance.Provided a simple, modular design (easy to generalize).\n",
        "6. Limitations\n",
        "- AlexNet:\n",
        " - Shallow by today’s standards (only 8 layers).Still had a very large number of parameters → prone to overfitting.Large filter sizes in early layers were less efficient.\n",
        "- VGGNet:\n",
        " - Extremely computationally expensive (138M parameters, ~500MB model size).\n",
        "Slow to train and memory intensive → not practical for deployment.\n",
        "Despite depth, lacked techniques like residual connections (introduced later in ResNet).\n",
        "\n",
        "Question 4: What is transfer learning in the context of image classification? Explain how it helps in reducing computational costs and improving model performance with limited data.\n",
        "- Transfer Learning is a deep learning technique where a model trained on a large dataset (e.g., ImageNet with millions of images) is reused (transferred) for a new but related task with a smaller dataset.Instead of training from scratch, we use the pre-trained model’s learned features and adapt (fine-tune) it for the new problem.\n",
        "- How It Works\n",
        "1. Pre-trained Model\n",
        "- A CNN (e.g., ResNet, VGG, Inception) trained on a large dataset learns general features such as edges, textures, shapes.\n",
        "2. Feature Reuse\n",
        "- These learned features are applicable to many vision tasks (e.g., cats vs. dogs, medical imaging).\n",
        "3. Fine-tuning / Feature Extraction\n",
        "- Feature Extraction: Freeze early layers, only train the classifier layers on new data.\n",
        "- Fine-Tuning: Unfreeze some deeper layers and retrain them for the new dataset.\n",
        "- How Transfer Learning Helps\n",
        "1. Reduces Computational Cost\n",
        "- Training large CNNs from scratch requires huge datasets and expensive GPUs. With transfer learning, only a small portion of the model is retrained, saving time and resources.\n",
        "2. Improves Performance with Limited Data\n",
        "- In many domains (e.g., medical imaging), labeled data is scarce.Transfer learning leverages knowledge from large datasets, boosting accuracy even with small datasets.\n",
        "3. Faster Convergence\n",
        "- Since the model already has good initial weights, training converges faster than training from scratch.\n",
        "4. Better Generalization\n",
        "- Pre-trained models provide robust, generalized feature representations, reducing overfitting when data is limited.\n",
        "- Example : A CNN trained on ImageNet (1,000 categories) can be fine-tuned to classify different flowers, medical scans, or animal species with only a few thousand images.\n",
        "\n",
        "Question 5: Describe the role of residual connections in ResNet architecture. How do they address the vanishing gradient problem in deep CNNs?   \n",
        "1. Background Problem : As CNNs got deeper (e.g., VGG with 19 layers), researchers noticed that simply stacking more layers:\n",
        " - Did not always improve accuracy : Often caused vanishing/exploding gradients, making training unstable.Training error sometimes increased with depth (degradation problem).\n",
        " - ResNet (Residual Network) Solution : Proposed by He et al. (2015) in “Deep Residual Learning for Image Recognition”.Introduced Residual Connections (or Skip Connections).\n",
        "- Role of Residual Connections\n",
        " - Definition\n",
        "  - A residual block learns:y=F(x)+x\n",
        "    - where:F(x) = transformation (convolution, batch norm, ReLU).\n",
        "    - x = input (added directly to the output).\n",
        " - Shortcut/Skip Path\n",
        "  - The input x bypasses (skips) one or more layers and is added to the output of those layers.If the learned mapping F(x) is zero, the block simply passes the input forward → acts like an identity mapping.\n",
        "- How They Address Vanishing Gradient Problem\n",
        " - Gradient Flow Improvement : In backpropagation, the gradient can flow directly through the skip connection, avoiding getting too small.Prevents gradients from vanishing in very deep networks.\n",
        " - Easier Optimization : Instead of forcing layers to learn the full mapping H(x), residual blocks learn the residual function F(x)=H(x)−x.Learning residuals is easier and more stable than learning direct mappings.\n",
        " - Training Very Deep Networks : ResNet successfully trained networks with 50, 101, and even 152 layers, which was impossible before due to vanishing gradients.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lqYch1PHbwPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Implement the LeNet-5 architectures using Tensorflow or PyTorch to classify the MNIST dataset. Report the accuracy and training time.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "# ---------------------------\n",
        "# LeNet-5 Model Definition\n",
        "# ---------------------------\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)   # C1: 1x28x28 -> 6x24x24\n",
        "        self.pool = nn.AvgPool2d(2, stride=2)         # S2: 6x24x24 -> 6x12x12\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # C3: 6x12x12 -> 16x8x8\n",
        "        # S4: 16x8x8 -> 16x4x4 after pooling\n",
        "        self.fc1 = nn.Linear(16*4*4, 120)             # C5\n",
        "        self.fc2 = nn.Linear(120, 84)                 # F6\n",
        "        self.fc3 = nn.Linear(84, 10)                  # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.tanh(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 16*4*4)  # Flatten\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = F.tanh(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Data Loading (MNIST)\n",
        "# ---------------------------\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Training\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LeNet5().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(5):  # 5 epochs\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation\n",
        "# ---------------------------\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkPi4yREHkg3",
        "outputId": "d99d8247-b781-4f1e-ce98-10f7ccf17565"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 34.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.03MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.29MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.14MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.3136\n",
            "Epoch 2, Loss: 0.1042\n",
            "Epoch 3, Loss: 0.0689\n",
            "Epoch 4, Loss: 0.0541\n",
            "Epoch 5, Loss: 0.0434\n",
            "\n",
            "Test Accuracy: 98.22%\n",
            "Training Time: 123.67 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Use a pre-trained VGG16 model (via transfer learning) on a small custom dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model. Include your code and result discussion.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import time\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load Pre-trained VGG16 (without top layers)\n",
        "# ---------------------------\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze base model layers (so they don't update during training)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Add Custom Classifier on Top\n",
        "# ---------------------------\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(5, activation='softmax')   # change \"5\" to number of classes in your dataset\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Data Preparation (Custom Dataset)\n",
        "# ---------------------------\n",
        "# Suppose you have folders: dataset/train & dataset/val with subfolders for each class\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=20,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dir = \"dataset/train\"   # <-- replace with your path\n",
        "val_dir = \"dataset/val\"       # <-- replace with your path\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir, target_size=(224, 224), batch_size=32, class_mode=\"categorical\")\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Training\n",
        "# ---------------------------\n",
        "start_time = time.time()\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Fine-Tuning (Unfreeze some deeper layers)\n",
        "# ---------------------------\n",
        "# Unfreeze last few convolutional blocks for fine-tuning\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-5),  # smaller LR\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_generator,\n",
        "    epochs=3,\n",
        "    validation_data=val_generator,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Evaluation\n",
        "# ---------------------------\n",
        "val_loss, val_acc = model.evaluate(val_generator, verbose=0)\n",
        "\n",
        "print(f\"\\nValidation Accuracy after Fine-Tuning: {val_acc*100:.2f}%\")\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "id": "Cr8pbjHBHzoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a program to visualize the filters and feature maps of the first convolutional layer of AlexNet on an example input image.\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Load Pre-trained AlexNet\n",
        "# ---------------------------\n",
        "alexnet = models.alexnet(pretrained=True)\n",
        "alexnet.eval()\n",
        "\n",
        "# First convolutional layer\n",
        "first_conv = alexnet.features[0]\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Load Example Image\n",
        "# ---------------------------\n",
        "img_path = \"example.jpg\"  # <-- replace with your image path\n",
        "image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "# Preprocess: resize, center crop, normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "img_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Visualize Filters (Weights of Conv1)\n",
        "# ---------------------------\n",
        "filters = first_conv.weight.data.clone()\n",
        "\n",
        "fig, axes = plt.subplots(4, 8, figsize=(12,6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < filters.shape[0]:\n",
        "        f = filters[i].cpu()\n",
        "        f = (f - f.min()) / (f.max() - f.min())  # normalize to [0,1]\n",
        "        ax.imshow(f.permute(1,2,0))\n",
        "        ax.axis(\"off\")\n",
        "plt.suptitle(\"First Convolutional Layer Filters (AlexNet)\")\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Visualize Feature Maps\n",
        "# ---------------------------\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(img_tensor)\n",
        "\n",
        "feature_maps = feature_maps.squeeze(0)  # remove batch dim\n",
        "\n",
        "fig, axes = plt.subplots(4, 8, figsize=(12,6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < feature_maps.shape[0]:\n",
        "        fmap = feature_maps[i].cpu()\n",
        "        fmap = (fmap - fmap.min()) / (fmap.max() - fmap.min())\n",
        "        ax.imshow(fmap, cmap=\"gray\")\n",
        "        ax.axis(\"off\")\n",
        "plt.suptitle(\"Feature Maps from First Convolutional Layer (AlexNet)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dLUP9R_YIGDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Train a GoogLeNet (Inception v1) or its variant using a standard dataset like CIFAR-10. Plot the training and validation accuracy over epochs and analyze overfitting or underfitting.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Device config\n",
        "# ---------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Data Preprocessing\n",
        "# ---------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Load GoogLeNet\n",
        "# ---------------------------\n",
        "from torchvision.models import googlenet\n",
        "\n",
        "net = googlenet(num_classes=10, aux_logits=True)  # CIFAR-10 has 10 classes\n",
        "net = net.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# 4. Loss & Optimizer\n",
        "# ---------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# ---------------------------\n",
        "# 5. Training Loop\n",
        "# ---------------------------\n",
        "num_epochs = 10\n",
        "train_acc_list, val_acc_list = [], []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, aux2, aux1 = net(inputs)  # GoogLeNet has 3 outputs\n",
        "        loss1 = criterion(outputs, labels)\n",
        "        loss2 = criterion(aux1, labels)\n",
        "        loss3 = criterion(aux2, labels)\n",
        "        loss = loss1 + 0.3*(loss2 + loss3)  # weighted loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_acc = 100.*correct/total\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    net.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100.*correct/total\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "          f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# ---------------------------\n",
        "# 6. Plot Accuracy\n",
        "# ---------------------------\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, num_epochs+1), train_acc_list, label=\"Training Accuracy\")\n",
        "plt.plot(range(1, num_epochs+1), val_acc_list, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"GoogLeNet on CIFAR-10\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdPHfxmPIQPm",
        "outputId": "1da0e6b3-cdb8-406f-ccfb-c46706ad6d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:01<00:00, 88.3MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: You are working in a healthcare AI startup. Your team is tasked with developing a system that automatically classifies medical X-ray images into normal, pneumonia, and COVID-19. Due to limited labeled data, what approach would you suggest using among CNN architectures discussed (e.g., transfer learning with ResNet or Inception variants)? Justify your approach and outline a deployment strategy for production use.\n",
        "# train_resnet_medxray.py\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import time\n",
        "import json\n",
        "\n",
        "# -----------------------\n",
        "# Config (edit as needed)\n",
        "# -----------------------\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 3\n",
        "TRAIN_DIR = \"dataset/train\"   # subfolders: normal/, pneumonia/, covid/\n",
        "VAL_DIR   = \"dataset/val\"\n",
        "TEST_DIR  = \"dataset/test\"\n",
        "OUTPUT_DIR = \"output_model\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "SEED = 42\n",
        "\n",
        "# -----------------------\n",
        "# Data generators / augmentation\n",
        "# -----------------------\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.08,\n",
        "    height_shift_range=0.08,\n",
        "    shear_range=0.05,\n",
        "    zoom_range=0.08,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=True, seed=SEED\n",
        ")\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    VAL_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False\n",
        ")\n",
        "test_gen = val_datagen.flow_from_directory(\n",
        "    TEST_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# Compute class weights to address imbalance\n",
        "# -----------------------\n",
        "labels_list = []\n",
        "for cls_idx, cls_name in enumerate(train_gen.class_indices):\n",
        "    pass\n",
        "# sklearn expects integer labels; we read filenames to get labels\n",
        "# Build labels array for class_weight calculation\n",
        "num_train_samples = train_gen.samples\n",
        "y_for_weights = np.zeros(num_train_samples, dtype=int)\n",
        "# iterate generator once (slow but okay for weight calc)\n",
        "i = 0\n",
        "for x_batch, y_batch in train_gen:\n",
        "    batch_size = y_batch.shape[0]\n",
        "    y_int = np.argmax(y_batch, axis=1)\n",
        "    y_for_weights[i:i+batch_size] = y_int\n",
        "    i += batch_size\n",
        "    if i >= num_train_samples:\n",
        "        break\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_for_weights), y=y_for_weights)\n",
        "class_weights = {i: w for i, w in enumerate(class_weights)}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Reset train_gen (generator internal state)\n",
        "train_gen.reset()\n",
        "\n",
        "# -----------------------\n",
        "# Build model (ResNet50 base)\n",
        "# -----------------------\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
        "base_model.trainable = False  # feature extraction phase\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -----------------------\n",
        "# Callbacks\n",
        "# -----------------------\n",
        "es = callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "mc = callbacks.ModelCheckpoint(os.path.join(OUTPUT_DIR, 'best_resnet50.h5'), monitor='val_loss', save_best_only=True)\n",
        "lr_cb = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "# -----------------------\n",
        "# Stage 1: Train top classifier (feature extraction)\n",
        "# -----------------------\n",
        "EPOCHS_FE = 10\n",
        "start = time.time()\n",
        "history_fe = model.fit(\n",
        "    train_gen,\n",
        "    epochs=EPOCHS_FE,\n",
        "    validation_data=val_gen,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[es, mc, lr_cb],\n",
        "    verbose=2\n",
        ")\n",
        "t_fe = time.time() - start\n",
        "print(f\"Feature-extraction training time: {t_fe:.1f}s\")\n",
        "\n",
        "# -----------------------\n",
        "# Stage 2: Fine-tuning - unfreeze some top layers\n",
        "# -----------------------\n",
        "# Unfreeze last conv block (example)\n",
        "base_model.trainable = True\n",
        "# Freeze earlier layers if memory / small data\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "EPOCHS_FT = 10\n",
        "start = time.time()\n",
        "history_ft = model.fit(\n",
        "    train_gen,\n",
        "    epochs=EPOCHS_FT,\n",
        "    validation_data=val_gen,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[es, mc, lr_cb],\n",
        "    verbose=2\n",
        ")\n",
        "t_ft = time.time() - start\n",
        "print(f\"Fine-tuning time: {t_ft:.1f}s\")\n",
        "\n",
        "# -----------------------\n",
        "# Evaluate on test set\n",
        "# -----------------------\n",
        "model.load_weights(os.path.join(OUTPUT_DIR, 'best_resnet50.h5'))\n",
        "test_loss, test_acc = model.evaluate(test_gen, verbose=1)\n",
        "print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "# Save final model and class indices\n",
        "model.save(os.path.join(OUTPUT_DIR, 'resnet50_med_xray_final.h5'))\n",
        "with open(os.path.join(OUTPUT_DIR, 'class_indices.json'), 'w') as f:\n",
        "    json.dump(train_gen.class_indices, f)\n",
        "\n",
        "# -----------------------\n",
        "# Temperature scaling (simple calibration on validation set)\n",
        "# -----------------------\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "# get logits on validation set\n",
        "val_gen.reset()\n",
        "probs = []\n",
        "labels = []\n",
        "for x_batch, y_batch in val_gen:\n",
        "    logits = model.predict(x_batch, verbose=0)  # softmax outputs\n",
        "    probs.append(logits)\n",
        "    labels.append(y_batch)\n",
        "    if len(probs)*BATCH_SIZE >= val_gen.samples:\n",
        "        break\n",
        "probs = np.vstack(probs)\n",
        "labels = np.vstack(labels)\n",
        "y_true = np.argmax(labels, axis=1)\n",
        "\n",
        "# temperature scaling: optimize temperature T to minimize NLL on val set\n",
        "logits = np.log(np.clip(probs, 1e-12, 1.0))  # approximate logits via log(probs)\n",
        "def nll_T(T):\n",
        "    T = T[0]\n",
        "    scaled = logits / T\n",
        "    scaled = np.exp(scaled - np.max(scaled, axis=1, keepdims=True))\n",
        "    scaled = scaled / scaled.sum(axis=1, keepdims=True)\n",
        "    # negative log-likelihood\n",
        "    ll = -np.sum(np.log(np.clip(scaled[np.arange(len(y_true)), y_true],1e-12,None)))\n",
        "    return ll\n",
        "\n",
        "res = minimize(nll_T, x0=[1.0], bounds=[(0.05, 10.0)])\n",
        "T_opt = res.x[0]\n",
        "print(\"Optimal temperature:\", T_opt)\n",
        "\n",
        "# save temperature\n",
        "with open(os.path.join(OUTPUT_DIR, 'temperature.txt'), 'w') as f:\n",
        "    f.write(str(float(T_opt)))\n"
      ],
      "metadata": {
        "id": "XY1Vj0g3Iccm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "# serve_model.py\n",
        "from flask import Flask, request, jsonify\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "MODEL_PATH = \"output_model/resnet50_med_xray_final.h5\"\n",
        "CLASS_INDICES = \"output_model/class_indices.json\"\n",
        "TEMP_PATH = \"output_model/temperature.txt\"\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "with open(CLASS_INDICES, 'r') as f:\n",
        "    class_indices = json.load(f)\n",
        "inv_class = {v:k for k,v in class_indices.items()}\n",
        "with open(TEMP_PATH,'r') as f:\n",
        "    temperature = float(f.read().strip())\n",
        "\n",
        "IMG_SIZE = (224,224)\n",
        "\n",
        "def preprocess_image(file_bytes):\n",
        "    image = Image.open(io.BytesIO(file_bytes)).convert('RGB').resize(IMG_SIZE)\n",
        "    arr = np.array(image)/255.0\n",
        "    arr = np.expand_dims(arr, 0)\n",
        "    return arr\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({'error':'no file uploaded'}), 400\n",
        "    file = request.files['file']\n",
        "    img = preprocess_image(file.read())\n",
        "    probs = model.predict(img)[0]\n",
        "    # temperature scaling\n",
        "    logits = np.log(np.clip(probs,1e-12,1.0))\n",
        "    scaled = np.exp(logits / temperature)\n",
        "    scaled = scaled / scaled.sum()\n",
        "    top_idx = int(np.argmax(scaled))\n",
        "    pred_class = inv_class[top_idx]\n",
        "    return jsonify({\n",
        "        'pred_class': pred_class,\n",
        "        'probabilities': {inv_class[i]: float(scaled[i]) for i in range(len(scaled))}\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "id": "kkvIsaxTJCNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10.\n",
        "1. Short recommendation & justification\n",
        "- Suggested approach (high level):Use transfer learning with a modern backbone such as ResNet50 or EfficientNetB0–B3, start with ImageNet weights, perform heavy domain-specific data augmentation, use class-imbalance handling (class weights or focal loss), calibrate and estimate uncertainty (e.g., temperature scaling, MC-dropout), and fine-tune the top convolutional blocks. Add explainability (Grad-CAM) and rigorous external/clinical validation before deployment.\n",
        "- Why this approach:Transfer learning from ImageNet is effective for medical imaging tasks when labeled data is limited — many reviews and empirical studies show robust performance gains vs training from scratch.ResNet variants (ResNet50/101) and EfficientNet family consistently perform well in chest X-ray classification and medical imaging tasks; some studies report EfficientNet and ResNet as top performers for chest radiographs.Model calibration and uncertainty matter in clinical settings — poorly calibrated probabilities can harm triage/decision making; incorporate calibration and uncertainty quantification in the pipeline.Regulatory & lifecycle expectations (validation, monitoring, change control) should be planned early — e.g., FDA draft guidance emphasizes lifecycle management and post-market monitoring for AI medical devices.\n",
        "2) Code — Transfer learning with ResNet50 (TensorFlow / Keras)\n",
        "This script is ready to run. It:loads images from train/, val/, test/ directories (each with subfolders normal/, pneumonia/, covid/),\n",
        "builds a ResNet50-based classifier,uses augmentation, class weights, and early stopping,performs initial training (feature extraction) then fine-tuning,\n",
        "saves the final model and shows how to apply temperature scaling for calibration.\n",
        "- Deployment strategy (production-ready checklist)\n",
        " - Data & Validation\n",
        "   - Curate diverse, de-identified datasets from multiple hospitals and imaging devices; split into training / internal validation / external validation sets. Perform statistical comparison of cohort distributions.\n",
        " - Model lifecycle & Regulatory\n",
        "   - Document dataset provenance, training hyperparameters, performance on held-out and external sets, and a clinical validation plan. Follow device lifecycle guidance (e.g., FDA AI/ML draft guidance). Plan for post-market monitoring and retraining procedures.\n",
        " - Performance & Safety\n",
        "  - Evaluate not only accuracy but sensitivity/recall for critical classes (e.g., COVID/pneumonia), false negative risks, and calibration/uncertainty for triage decisions. Include a referral threshold: low-confidence predictions should be sent to a radiologist.\n",
        " - Explainability & Clinician UX\n",
        "  - Provide visual explanations (Grad-CAM saliency maps) alongside predictions to help clinicians interpret model focus areas. This increases trust and helps detect dataset/domain shifts.\n",
        " - Robustness & Monitoring\n",
        "  - Run continuous performance monitoring: track input distribution drift, model accuracy trends, and triggered alerts if performance drops. Implement pipelines for safe retraining and versioning (MLOps).\n",
        " - Serving & Scalability\n",
        "  - For production: use TF-Serving / TorchServe behind an API gateway, containerized (Docker), orchestrated (Kubernetes). Use GPU nodes for batch inference; autoscale for peak loads. Use TLS, authentication, logging, and audit trails.Implement asynchronous pipelines where human review is required but do not auto-treat patients based on model output alone without clinical oversight.\n",
        " - Human-in-the-loop & Fail-safes\n",
        "  - Require clinician confirmation for action, and route uncertain predictions to specialists. Keep an easy way to collect feedback labels for continuous improvement.\n",
        "5) Practical tips, pitfalls & further improvements\n",
        "- Try EfficientNet family as an alternative backbone for better parameter efficiency — many recent papers report strong performance on chest X-rays.\n",
        "Use cross-validation and external test sets from different hospitals for realistic generalization estimates.Consider ensembles or model-agnostic uncertainty (ensembles, MC-dropout) to improve reliability.Always measure calibration (ECE, NLL) and perform post-hoc calibration if needed."
      ],
      "metadata": {
        "id": "1KtNaNZhJGSu"
      }
    }
  ]
}